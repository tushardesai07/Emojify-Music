{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "372e439a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D False\n",
      "2 BatchNormalization False\n",
      "3 ReLU False\n",
      "4 DepthwiseConv2D False\n",
      "5 BatchNormalization False\n",
      "6 ReLU False\n",
      "7 Conv2D False\n",
      "8 BatchNormalization False\n",
      "9 ReLU False\n",
      "10 ZeroPadding2D False\n",
      "11 DepthwiseConv2D False\n",
      "12 BatchNormalization False\n",
      "13 ReLU False\n",
      "14 Conv2D False\n",
      "15 BatchNormalization False\n",
      "16 ReLU False\n",
      "17 DepthwiseConv2D False\n",
      "18 BatchNormalization False\n",
      "19 ReLU False\n",
      "20 Conv2D False\n",
      "21 BatchNormalization False\n",
      "22 ReLU False\n",
      "23 ZeroPadding2D False\n",
      "24 DepthwiseConv2D False\n",
      "25 BatchNormalization False\n",
      "26 ReLU False\n",
      "27 Conv2D False\n",
      "28 BatchNormalization False\n",
      "29 ReLU False\n",
      "30 DepthwiseConv2D False\n",
      "31 BatchNormalization False\n",
      "32 ReLU False\n",
      "33 Conv2D False\n",
      "34 BatchNormalization False\n",
      "35 ReLU False\n",
      "36 ZeroPadding2D False\n",
      "37 DepthwiseConv2D False\n",
      "38 BatchNormalization False\n",
      "39 ReLU False\n",
      "40 Conv2D False\n",
      "41 BatchNormalization False\n",
      "42 ReLU False\n",
      "43 DepthwiseConv2D False\n",
      "44 BatchNormalization False\n",
      "45 ReLU False\n",
      "46 Conv2D False\n",
      "47 BatchNormalization False\n",
      "48 ReLU False\n",
      "49 DepthwiseConv2D False\n",
      "50 BatchNormalization False\n",
      "51 ReLU False\n",
      "52 Conv2D False\n",
      "53 BatchNormalization False\n",
      "54 ReLU False\n",
      "55 DepthwiseConv2D False\n",
      "56 BatchNormalization False\n",
      "57 ReLU False\n",
      "58 Conv2D False\n",
      "59 BatchNormalization False\n",
      "60 ReLU False\n",
      "61 DepthwiseConv2D False\n",
      "62 BatchNormalization False\n",
      "63 ReLU False\n",
      "64 Conv2D False\n",
      "65 BatchNormalization False\n",
      "66 ReLU False\n",
      "67 DepthwiseConv2D False\n",
      "68 BatchNormalization False\n",
      "69 ReLU False\n",
      "70 Conv2D False\n",
      "71 BatchNormalization False\n",
      "72 ReLU False\n",
      "73 ZeroPadding2D False\n",
      "74 DepthwiseConv2D False\n",
      "75 BatchNormalization False\n",
      "76 ReLU False\n",
      "77 Conv2D False\n",
      "78 BatchNormalization False\n",
      "79 ReLU False\n",
      "80 DepthwiseConv2D False\n",
      "81 BatchNormalization False\n",
      "82 ReLU False\n",
      "83 Conv2D False\n",
      "84 BatchNormalization False\n",
      "85 ReLU False\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 112, 112, 32)      864       \n",
      "                                                                 \n",
      " conv1_bn (BatchNormalizatio  (None, 112, 112, 32)     128       \n",
      " n)                                                              \n",
      "                                                                 \n",
      " conv1_relu (ReLU)           (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)     288       \n",
      "                                                                 \n",
      " conv_dw_1_bn (BatchNormaliz  (None, 112, 112, 32)     128       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_1_relu (ReLU)       (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      " conv_pw_1 (Conv2D)          (None, 112, 112, 64)      2048      \n",
      "                                                                 \n",
      " conv_pw_1_bn (BatchNormaliz  (None, 112, 112, 64)     256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_1_relu (ReLU)       (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv_pad_2 (ZeroPadding2D)  (None, 113, 113, 64)      0         \n",
      "                                                                 \n",
      " conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)       576       \n",
      "                                                                 \n",
      " conv_dw_2_bn (BatchNormaliz  (None, 56, 56, 64)       256       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_2_relu (ReLU)       (None, 56, 56, 64)        0         \n",
      "                                                                 \n",
      " conv_pw_2 (Conv2D)          (None, 56, 56, 128)       8192      \n",
      "                                                                 \n",
      " conv_pw_2_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_2_relu (ReLU)       (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)      1152      \n",
      "                                                                 \n",
      " conv_dw_3_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv_pw_3 (Conv2D)          (None, 56, 56, 128)       16384     \n",
      "                                                                 \n",
      " conv_pw_3_bn (BatchNormaliz  (None, 56, 56, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv_pad_4 (ZeroPadding2D)  (None, 57, 57, 128)       0         \n",
      "                                                                 \n",
      " conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)      1152      \n",
      "                                                                 \n",
      " conv_dw_4_bn (BatchNormaliz  (None, 28, 28, 128)      512       \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_4_relu (ReLU)       (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " conv_pw_4 (Conv2D)          (None, 28, 28, 256)       32768     \n",
      "                                                                 \n",
      " conv_pw_4_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_4_relu (ReLU)       (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)      2304      \n",
      "                                                                 \n",
      " conv_dw_5_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv_pw_5 (Conv2D)          (None, 28, 28, 256)       65536     \n",
      "                                                                 \n",
      " conv_pw_5_bn (BatchNormaliz  (None, 28, 28, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv_pad_6 (ZeroPadding2D)  (None, 29, 29, 256)       0         \n",
      "                                                                 \n",
      " conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)      2304      \n",
      "                                                                 \n",
      " conv_dw_6_bn (BatchNormaliz  (None, 14, 14, 256)      1024      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_6_relu (ReLU)       (None, 14, 14, 256)       0         \n",
      "                                                                 \n",
      " conv_pw_6 (Conv2D)          (None, 14, 14, 512)       131072    \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv_pw_6_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_6_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
      "                                                                 \n",
      " conv_dw_7_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_7 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_7_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
      "                                                                 \n",
      " conv_dw_8_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_8 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_8_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)      4608      \n",
      "                                                                 \n",
      " conv_dw_9_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_dw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_9 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_9_bn (BatchNormaliz  (None, 14, 14, 512)      2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " conv_pw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_10 (DepthwiseConv2D  (None, 14, 14, 512)      4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_10_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_10 (Conv2D)         (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_10_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_11 (DepthwiseConv2D  (None, 14, 14, 512)      4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_11_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_11 (Conv2D)         (None, 14, 14, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_11_bn (BatchNormali  (None, 14, 14, 512)      2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)      0         \n",
      "                                                                 \n",
      " conv_dw_12 (DepthwiseConv2D  (None, 7, 7, 512)        4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_12_bn (BatchNormali  (None, 7, 7, 512)        2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_12_relu (ReLU)      (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv_pw_12 (Conv2D)         (None, 7, 7, 1024)        524288    \n",
      "                                                                 \n",
      " conv_pw_12_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_12_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " conv_dw_13 (DepthwiseConv2D  (None, 7, 7, 1024)       9216      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_13_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " conv_pw_13 (Conv2D)         (None, 7, 7, 1024)        1048576   \n",
      "                                                                 \n",
      " conv_pw_13_bn (BatchNormali  (None, 7, 7, 1024)       4096      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1024)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,856,455\n",
      "Trainable params: 2,627,591\n",
      "Non-trainable params: 3,228,864\n",
      "_________________________________________________________________\n",
      "None\n",
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RHUNALI\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.6415 - accuracy: 0.3528\n",
      "Epoch 1: val_loss improved from inf to 1.50000, saving model to model.h5\n",
      "625/625 [==============================] - 1938s 3s/step - loss: 1.6415 - accuracy: 0.3528 - val_loss: 1.5000 - val_accuracy: 0.4229 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.5242 - accuracy: 0.4066\n",
      "Epoch 2: val_loss improved from 1.50000 to 1.42140, saving model to model.h5\n",
      "625/625 [==============================] - 1657s 3s/step - loss: 1.5242 - accuracy: 0.4066 - val_loss: 1.4214 - val_accuracy: 0.4464 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.4938 - accuracy: 0.4190 \n",
      "Epoch 3: val_loss did not improve from 1.42140\n",
      "625/625 [==============================] - 16056s 26s/step - loss: 1.4938 - accuracy: 0.4190 - val_loss: 1.4731 - val_accuracy: 0.4435 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.4661 - accuracy: 0.4327\n",
      "Epoch 4: val_loss improved from 1.42140 to 1.41077, saving model to model.h5\n",
      "625/625 [==============================] - 380s 608ms/step - loss: 1.4661 - accuracy: 0.4327 - val_loss: 1.4108 - val_accuracy: 0.4696 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.4570 - accuracy: 0.4347\n",
      "Epoch 5: val_loss improved from 1.41077 to 1.37981, saving model to model.h5\n",
      "625/625 [==============================] - 384s 615ms/step - loss: 1.4570 - accuracy: 0.4347 - val_loss: 1.3798 - val_accuracy: 0.4790 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.4430 - accuracy: 0.4429\n",
      "Epoch 6: val_loss did not improve from 1.37981\n",
      "625/625 [==============================] - 372s 595ms/step - loss: 1.4430 - accuracy: 0.4429 - val_loss: 1.3857 - val_accuracy: 0.4629 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.4381 - accuracy: 0.4439\n",
      "Epoch 7: val_loss improved from 1.37981 to 1.35131, saving model to model.h5\n",
      "625/625 [==============================] - 534s 854ms/step - loss: 1.4381 - accuracy: 0.4439 - val_loss: 1.3513 - val_accuracy: 0.4819 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.4235 - accuracy: 0.4516\n",
      "Epoch 8: val_loss did not improve from 1.35131\n",
      "625/625 [==============================] - 512s 819ms/step - loss: 1.4235 - accuracy: 0.4516 - val_loss: 1.3932 - val_accuracy: 0.4682 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.4133 - accuracy: 0.4541\n",
      "Epoch 9: val_loss improved from 1.35131 to 1.32253, saving model to model.h5\n",
      "625/625 [==============================] - 421s 673ms/step - loss: 1.4133 - accuracy: 0.4541 - val_loss: 1.3225 - val_accuracy: 0.4993 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - ETA: 0s - loss: 1.4058 - accuracy: 0.4579\n",
      "Epoch 10: val_loss did not improve from 1.32253\n",
      "625/625 [==============================] - 400s 641ms/step - loss: 1.4058 - accuracy: 0.4579 - val_loss: 1.3581 - val_accuracy: 0.4857 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 27). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MyModel_tf\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./MyModel_tf\\assets\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import MobileNet\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_labels = 7  #number of labels or moods to be detected\n",
    "\n",
    "#MobileNet is designed to work with images of dim 224,224\n",
    "img_rows, img_cols = 224, 224\n",
    "\n",
    "#MobileNet model is defined and initialized\n",
    "MobileNet = MobileNet(weights='imagenet', include_top=False, input_shape=(img_rows, img_cols, 3))\n",
    "\n",
    "#Layers are set to trainable as True by default\n",
    "for layer in MobileNet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#All of the layers in MobileNet model are printed along with Boolean value True or False\n",
    "for (i, layer) in enumerate(MobileNet.layers):\n",
    "    print(str(i), layer.__class__.__name__, layer.trainable)\n",
    "\n",
    "#creates the top or head of the model that will be  placed ontop of the bottom layers\n",
    "def addTopModelMobileNet(bottom_model, num_classes):\n",
    "\n",
    "    top_model = bottom_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Dense(1024, activation='relu')(top_model)\n",
    "    top_model = Dense(1024, activation='relu')(top_model)\n",
    "    top_model = Dense(512, activation='relu')(top_model)\n",
    "    top_model = Dense(num_labels, activation='softmax')(top_model)\n",
    "\n",
    "    return top_model\n",
    "\n",
    "\n",
    "#connecting all of the layers with the bottom model\n",
    "FC_Head = addTopModelMobileNet(MobileNet, num_labels)\n",
    "\n",
    "#creating the model with MobileNet as the bottom model\n",
    "model = Model(inputs=MobileNet.input, outputs=FC_Head)\n",
    "\n",
    "print(model.summary())\n",
    "#this prints the layers, their types, their output shapes and the number of learnable parameters\n",
    "\n",
    "#path for training and validation data batches to be stored\n",
    "train_data_dir = 'archive/train'\n",
    "validation_data_dir = 'archive/test'\n",
    "\n",
    "#used to generate batches of tensor image data with real-time data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    rotation_range=30,\n",
    "                    width_shift_range=0.3,\n",
    "                    height_shift_range=0.3,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "\n",
    "#resizing validation images\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#setting batch size\n",
    "batch_size = 32\n",
    "\n",
    "#taking data from the specified path in specified batch size for training the model\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                        directory=train_data_dir,\n",
    "                        target_size=(img_rows, img_cols),\n",
    "                        batch_size=batch_size,\n",
    "                        class_mode=\"categorical\")\n",
    "\n",
    "#taking data from the specified path in specified batch size for validating the model\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "                            directory=validation_data_dir,\n",
    "                            target_size=(img_rows, img_cols),\n",
    "                            batch_size=batch_size,\n",
    "                            class_mode=\"categorical\")\n",
    "\n",
    "\n",
    "#saving changes after every interval\n",
    "checkpoint = ModelCheckpoint('model.h5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "#stopping the training when monitored metric has stopped improving\n",
    "earlystop = EarlyStopping(\n",
    "                          monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=10,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "#reduce learning rate of model if metric stopped improving after certain epochs\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                            patience=5,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.2,\n",
    "                                            min_lr=0.0001)\n",
    "\n",
    "#called at each stage of the training\n",
    "callbacks = [earlystop,checkpoint,learning_rate_reduction]\n",
    "\n",
    "#configures model for training\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nb_train_samples = 40045\n",
    "nb_validation_samples = 11924\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "#trains model on fixed number of epochs\n",
    "history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=nb_train_samples//(batch_size*2),\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_samples//(2*batch_size))\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "#plt.figure(figsize=(8, 8))\n",
    "#plt.subplot(1, 2, 1)\n",
    "#plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "#plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "#plt.legend(loc='lower right')\n",
    "#plt.title('Training and Validation Accuracy')\n",
    "\n",
    "#plt.subplot(1, 2, 2)\n",
    "###plt.plot(epochs_range, loss, label='Training Loss')\n",
    "#plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "#plt.legend(loc='upper right')\n",
    "#plt.title('Training and Validation Loss')\n",
    "#plt.show()\n",
    "\n",
    "fer_json = model.to_json()\n",
    "with open(\"new_model.json\", \"w\") as json_file:\n",
    "    json_file.write(fer_json)\n",
    "model.save('./MyModel_tf',save_format='tf')\n",
    "model.save(\"new_model.h5\")\n",
    "model.save_weights(\"new_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92fe0604",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16152\\2007551792.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'No Face Found'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Emotion Detector'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#display the frame and label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xFF\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'q'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import load_model\n",
    "from time import sleep\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "#from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "classifier =load_model('./model.h5') #loading the trained model\n",
    "\n",
    "class_labels = ['Angry', 'Disgust', 'Fear', 'Happy','Neutral','Sad','Surprise'] #defining labels\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW) #real-time detection of the emotions\n",
    "cap=cv2.VideoCapture(2, cv2.CAP_DSHOW)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = cap.read()\n",
    "    labels = []\n",
    "    #gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #colortograyscale\n",
    "    faces = face_classifier.detectMultiScale(frame,1.3,5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2) #drawing rectangle around face\n",
    "        roi_gray = frame[y:y+h,x:x+w]\n",
    "        #roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "        if np.sum([roi_gray])!=0:\n",
    "            roi = roi_gray.astype('float')/255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi,axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "\n",
    "            preds = classifier.predict(roi)[0]\n",
    "            print(\"\\nprediction = \",preds)\n",
    "            label=class_labels[preds.argmax()]\n",
    "            print(\"\\nprediction max = \",preds.argmax())\n",
    "            print(\"\\nlabel = \",label)\n",
    "            label_position = (x,y) #setting position of the label\n",
    "            cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "        else:\n",
    "            cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "        print(\"\\n\\n\")\n",
    "    cv2.imshow('Emotion Detector',frame) #display the frame and label\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8504983f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping opencv-python-headless as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\rhunali\\anaconda3\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\rhunali\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall opencv-python-headless -y \n",
    "\n",
    "!pip install opencv-python --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a830a9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip install [options] [-e] <vcs project url> ...\n",
      "  pip install [options] [-e] <local project path> ...\n",
      "  pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -y\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "578ee5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.7.0.72-cp37-abi3-win_amd64.whl (38.1 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\rhunali\\anaconda3\\lib\\site-packages (from opencv-python-headless) (1.21.5)\n",
      "Installing collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.7.0.72\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless --user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "556bf8bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16152\\3975407133.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#colortograyscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import load_model\n",
    "from time import sleep\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "#from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "classifier =load_model('./model.h5') #loading the trained model\n",
    "\n",
    "class_labels = ['Angry', 'Disgust', 'Fear', 'Happy','Neutral','Sad','Surprise'] #defining labels\n",
    "\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW) #real-time detection of the emotions\n",
    "cap=cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = cap.read()\n",
    "    labels = []\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #colortograyscale\n",
    "    faces = face_classifier.detectMultiScale(frame,1.3,5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2) #drawing rectangle around face\n",
    "        roi_gray = frame[y:y+h,x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "        if np.sum([roi_gray])!=0:\n",
    "            roi = roi_gray.astype('float')/255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi,axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "\n",
    "            preds = classifier.predict(roi)[0]\n",
    "            print(\"\\nprediction = \",preds)\n",
    "            label=class_labels[preds.argmax()]\n",
    "            print(\"\\nprediction max = \",preds.argmax())\n",
    "            print(\"\\nlabel = \",label)\n",
    "            label_position = (x,y) #setting position of the label\n",
    "            cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "        else:\n",
    "            cv2.putText(frame,'No Face Found',(20,60),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\n",
    "        print(\"\\n\\n\")\n",
    "    cv2.imshow('Emotion Detector',frame) #display the frame and label\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce72299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb36c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32984319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f62f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
